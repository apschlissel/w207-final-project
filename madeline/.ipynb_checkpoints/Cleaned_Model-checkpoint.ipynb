{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b45e3020",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, sys\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from PIL import Image, ImageOps\n",
    "from pathlib import Path\n",
    "from keras.preprocessing.image import img_to_array\n",
    "\n",
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, sys\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from PIL import Image, ImageOps\n",
    "from pathlib import Path\n",
    "\n",
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D , MaxPool2D , Flatten , Dropout \n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "#from keras.optimizers import Adam\n",
    "\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import metrics\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten \n",
    "from keras.layers import MaxPooling2D, Dropout\n",
    "\n",
    "from scipy.ndimage import gaussian_filter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a6d1a8",
   "metadata": {},
   "source": [
    "# Data Cleaning & Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "b06d2cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Shape: (9912, 20, 20, 3)\n",
      "Labels Shape; (9912,)\n"
     ]
    }
   ],
   "source": [
    "# importing and resizing image data \n",
    "# change path to wherever your data lives on your computer\n",
    "csv_path = \"/Users/madelinewhitlow/Desktop/MIDS_W207/petfinder-pawpularity-score/train.csv\"\n",
    "image_path = '/Users/madelinewhitlow/Desktop/MIDS_W207/petfinder-pawpularity-score/train/'\n",
    "\n",
    "def CutenessBuckets(pawpularity_score):\n",
    "    if pawpularity_score <= 10:\n",
    "        return 0\n",
    "    elif pawpularity_score <= 20:\n",
    "        return 1\n",
    "    elif pawpularity_score <= 30:\n",
    "        return 2\n",
    "    elif pawpularity_score <= 40:\n",
    "        return 3\n",
    "    elif pawpularity_score <= 50:\n",
    "        return 4\n",
    "    elif pawpularity_score <= 60:\n",
    "        return 5\n",
    "    elif pawpularity_score <= 70:\n",
    "        return 6\n",
    "    elif pawpularity_score <= 80:\n",
    "        return 7\n",
    "    elif pawpularity_score <= 90:\n",
    "        return 8\n",
    "    elif pawpularity_score <= 100:\n",
    "        return 9\n",
    "\n",
    "def CleanandProcessData(csv_path, image_path, image_resize):\n",
    "\n",
    "    # imnporting csv with image ids and pawpularity scores\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # creating path to image using the id in the data frame we've created\n",
    "    df['path'] = image_path + df['Id'] + '.jpg'\n",
    "    df['img'] = df['path'].apply(lambda x: Image.open(x).convert(\"RGB\").resize((image_resize,image_resize)))\n",
    "    \n",
    "    df['cuteness_bin'] = df['Pawpularity'].apply(CutenessBuckets)\n",
    "    df['img_array'] = df['img'].apply(lambda x: img_to_array(x)/255)\n",
    "\n",
    "    labels_raw=[]\n",
    "    labels_bin = []\n",
    "    imgArr=[]\n",
    "\n",
    "    for ind in df.index:\n",
    "        img = df.img_array[0]\n",
    "        imgArr.append(img)\n",
    "        labels_raw.append(df['Pawpularity'][ind])\n",
    "        labels_bin.append(df['cuteness_bin'][ind])\n",
    "\n",
    "    labels_raw = np.array(labels_raw)\n",
    "    labels_bin = np.array(labels_bin)\n",
    "    X_train=np.array(imgArr, dtype=float)\n",
    "    print('Training Shape:',X_train.shape)\n",
    "    print('Labels Shape;', labels.shape)\n",
    "    return X_train, labels_raw, labels_bin\n",
    "\n",
    "X_train, labels_raw, labels_bin = CleanandProcessData(csv_path, image_path, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "600ca3bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape:  (9912, 20, 20, 3)\n",
      "pawpularity label shape: (9912,)\n",
      "bin label shape: (9912,)\n"
     ]
    }
   ],
   "source": [
    "# Shuffling data and separating into train, test and dev sets\n",
    "shuffle = np.random.permutation(np.arange(X_train.shape[0]))\n",
    "\n",
    "X_train, labels_raw, labels_bin = X_train[shuffle], labels_raw[shuffle], labels_bin[shuffle]\n",
    "\n",
    "print('data shape: ', X_train.shape)\n",
    "print('pawpularity label shape:', labels_raw.shape)\n",
    "print('bin label shape:', labels_bin.shape)\n",
    "\n",
    "# Set some variables to hold test, dev, and training data.\n",
    "train_data, train_labels_raw, train_labels_bin = X_train[:7921], labels_raw[:7921], labels_bin[:7921]\n",
    "test_data, test_labels_raw, test_labels_bin = X_train[8916:], labels_raw[8916:], labels_bin[8916:]\n",
    "dev_data, dev_labels_raw, dev_labels_bin = X_train[7920:8916], labels_raw[7920:8916], labels_bin[7920:8916]\n",
    "\n",
    "# raw labels (ie original pawpularity score)\n",
    "test_labels_category = to_categorical(test_labels_raw)\n",
    "train_labels_category = to_categorical(train_labels_raw)\n",
    "dev_labels_category = to_categorical(dev_labels_raw)\n",
    "\n",
    "# bucketed labels \n",
    "test_labels_bins_category = to_categorical(test_labels_bin)\n",
    "train_labels_bins_category = to_categorical(train_labels_bin)\n",
    "dev_labels_bins_category = to_categorical(dev_labels_bin)\n",
    "\n",
    "# blurred image data (optional use)\n",
    "blurred_train = gaussian_filter(train_data, sigma=1)\n",
    "blurred_test = gaussian_filter(test_data, sigma = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992ee56d",
   "metadata": {},
   "source": [
    "# First CCN Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "7e3e2d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- CNN Model 1 with all Paw scores ----------\n",
      "Epoch 1/3\n",
      "248/248 [==============================] - 4s 13ms/step - loss: 4.2646 - accuracy: 0.0276 - val_loss: 4.1839 - val_accuracy: 0.0371\n",
      "Epoch 2/3\n",
      "248/248 [==============================] - 3s 13ms/step - loss: 4.2201 - accuracy: 0.0274 - val_loss: 4.1711 - val_accuracy: 0.0371\n",
      "Epoch 3/3\n",
      "248/248 [==============================] - 3s 13ms/step - loss: 4.2177 - accuracy: 0.0290 - val_loss: 4.1668 - val_accuracy: 0.0371\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 4.1668 - accuracy: 0.0371\n",
      "\n",
      "Test loss: 4.166837692260742\n",
      "Test accuracy: 0.03714859485626221\n"
     ]
    }
   ],
   "source": [
    "shape = (train_data.shape[1],train_data.shape[2], train_data.shape[3])\n",
    "possible_outcomes = 101\n",
    "# first model attempt (using raw pawpularity scores)\n",
    "# switch out activation to 'sigmoid'\n",
    "# try different pixel amounts (100x100) (20x20) etc\n",
    "#try binary crosstentropy vs categorical \n",
    "#adjust epochs\n",
    "\n",
    "\n",
    "model = Sequential()#add model layers\n",
    "model.add(Conv2D(32, kernel_size=(5, 5),\n",
    "                     activation='relu',\n",
    "                     input_shape=shape))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# add second convolutional layer with 20 filters\n",
    "model.add(Conv2D(64, (5, 5), activation='relu'))\n",
    "    \n",
    "# add 2D pooling layer\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "# flatten data\n",
    "model.add(Flatten())\n",
    "    \n",
    "# add a dense all-to-all relu layer\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "    \n",
    "# apply dropout with rate 0.5\n",
    "model.add(Dropout(0.5))\n",
    "    \n",
    "# soft-max layer\n",
    "model.add(Dense(possible_outcomes, activation='softmax'))\n",
    "#compile model using accuracy to measure model performance\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#train the model\n",
    "\n",
    "print('-'*10, 'CNN Model 1 with all Paw scores', '-'*10)\n",
    "model.fit(train_data,train_labels_category, validation_data=(test_data, test_labels_category), epochs=3)\n",
    "# evaluate the model\n",
    "score = model.evaluate(test_data, test_labels_category, verbose=1)\n",
    "# print performance\n",
    "print()\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1119c77f",
   "metadata": {},
   "source": [
    "# Same Model but using 10 category buckets for scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "f93f6f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- CNN Model 1 with all Paw scores ----------\n",
      "Epoch 1/3\n",
      "248/248 [==============================] - 4s 15ms/step - loss: 1.9893 - accuracy: 0.2630 - val_loss: 1.9566 - val_accuracy: 0.2741\n",
      "Epoch 2/3\n",
      "248/248 [==============================] - 3s 11ms/step - loss: 1.9794 - accuracy: 0.2718 - val_loss: 1.9935 - val_accuracy: 0.2741\n",
      "Epoch 3/3\n",
      "248/248 [==============================] - 3s 10ms/step - loss: 1.9755 - accuracy: 0.2767 - val_loss: 1.9479 - val_accuracy: 0.2741\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 1.9479 - accuracy: 0.2741\n",
      "\n",
      "Test loss: 1.9479047060012817\n",
      "Test accuracy: 0.27409639954566956\n"
     ]
    }
   ],
   "source": [
    "shape = (train_data.shape[1],train_data.shape[2], train_data.shape[3])\n",
    "possible_outcomes = 10\n",
    "# first model attempt (using raw pawpularity scores)\n",
    "# switch out activation to 'sigmoid'\n",
    "# try different pixel amounts (100x100) (20x20) etc\n",
    "#try binary crosstentropy vs categorical \n",
    "#adjust epochs\n",
    "\n",
    "\n",
    "model = Sequential()#add model layers\n",
    "model.add(Conv2D(32, kernel_size=(5, 5),\n",
    "                     activation='relu',\n",
    "                     input_shape=shape))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# add second convolutional layer with 20 filters\n",
    "model.add(Conv2D(64, (5, 5), activation='relu'))\n",
    "    \n",
    "# add 2D pooling layer\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "# flatten data\n",
    "model.add(Flatten())\n",
    "    \n",
    "# add a dense all-to-all relu layer\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "    \n",
    "# apply dropout with rate 0.5\n",
    "model.add(Dropout(0.5))\n",
    "    \n",
    "# soft-max layer\n",
    "model.add(Dense(possible_outcomes, activation='softmax'))\n",
    "#compile model using accuracy to measure model performance\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#train the model\n",
    "\n",
    "print('-'*10, 'CNN Model 1 with all Paw scores', '-'*10)\n",
    "model.fit(train_data,train_labels_bins_category, validation_data=(test_data, test_labels_bins_category), epochs=3)\n",
    "# evaluate the model\n",
    "score = model.evaluate(test_data, test_labels_bins_category, verbose=1)\n",
    "# print performance\n",
    "print()\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795c8e2d",
   "metadata": {},
   "source": [
    "# Second CNN Model - Using raw paw scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "f0dfc208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "247/247 [==============================] - 9s 31ms/step - loss: 0.4597 - rmse: 0.5130 - mae: 0.3820 - mape: 372008864.0000 - val_loss: 0.2012 - val_rmse: 0.1081 - val_mae: 0.0621 - val_mape: 52739960.0000\n",
      "Epoch 2/3\n",
      "247/247 [==============================] - 7s 27ms/step - loss: 0.2352 - rmse: 0.2199 - mae: 0.1581 - mape: 148278960.0000 - val_loss: 0.1958 - val_rmse: 0.1086 - val_mae: 0.0633 - val_mape: 53961764.0000\n",
      "Epoch 3/3\n",
      "247/247 [==============================] - 7s 27ms/step - loss: 0.2014 - rmse: 0.1430 - mae: 0.0889 - mape: 79061640.0000 - val_loss: 0.1875 - val_rmse: 0.0991 - val_mae: 0.0159 - val_mape: 6022475.5000\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.1875 - rmse: 0.0991 - mae: 0.0159 - mape: 6022475.5000\n",
      "Test accuracy: 0.09908347576856613\n"
     ]
    }
   ],
   "source": [
    "# model from kaggle page \n",
    "# https://www.kaggle.com/alexteboul/tutorial-part-3-cnn-image-modeling-1\n",
    "\n",
    "# this accuracy seems ot be higher we could mess around with different filters/padding etc\n",
    "shape = (train_data.shape[1],train_data.shape[2], train_data.shape[3])\n",
    "inputs = tf.keras.Input(shape=shape)\n",
    "\n",
    "#start off with x just being those inputs\n",
    "x = inputs\n",
    "\n",
    "x = tf.keras.layers.Conv2D(filters = 16, kernel_size = (7,7), strides = (2,2), padding='valid', kernel_initializer='he_normal', kernel_regularizer=l2(0.0005), activation = 'relu')(x)\n",
    "x = tf.keras.layers.Conv2D(filters = 32, kernel_size = (3,3), padding='same', activation = 'relu')(x)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "x = tf.keras.layers.Conv2D(filters = 32, kernel_size = (3,3), strides = (2,2), padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(0.0005), activation = 'relu')(x)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "x = tf.keras.layers.Dropout(0.25)(x)\n",
    "\n",
    "\n",
    "x = tf.keras.layers.Conv2D(filters = 64, kernel_size = (3,3), padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(0.0002), activation = 'relu')(x)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "x = tf.keras.layers.Conv2D(filters = 64, kernel_size = (3,3), strides = (2,2), padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(0.0005), activation = 'relu')(x)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "x = tf.keras.layers.Dropout(0.25)(x)\n",
    "\n",
    "x = tf.keras.layers.Conv2D(filters = 128, kernel_size = (3,3), padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(0.0002), activation = 'relu')(x)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = tf.keras.layers.Conv2D(filters = 128, kernel_size = (3,3),padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(0.0002), activation = 'relu')(x)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "x = tf.keras.layers.Dropout(0.25)(x)\n",
    "\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "x = tf.keras.layers.Dense(512, activation = \"relu\")(x)\n",
    "x = tf.keras.layers.Dropout(0.5)(x)\n",
    "\n",
    "output = tf.keras.layers.Dense(1)(x)\n",
    "\n",
    "model = tf.keras.Model(inputs = inputs, outputs = output)\n",
    "\n",
    "model.compile(\n",
    "    loss = 'mse', \n",
    "    optimizer = 'Adam', \n",
    "    metrics = [tf.keras.metrics.RootMeanSquaredError(name=\"rmse\"), \"mae\", \"mape\"])\n",
    "\n",
    "data_augmentation = ImageDataGenerator(\n",
    "    rotation_range = 15, \n",
    "    zoom_range = 0.15,\n",
    "    width_shift_range = 0.2, \n",
    "    height_shift_range = 0.2, \n",
    "    shear_range = 0.1,\n",
    "    horizontal_flip = True, \n",
    "    fill_mode = \"nearest\")\n",
    "history = model.fit(\n",
    "    data_augmentation.flow(train_data,train_labels_category,batch_size=32),\n",
    "    validation_data = (test_data, test_labels_category),\n",
    "    steps_per_epoch = len(blurred) // 32,\n",
    "    epochs = 3\n",
    ")\n",
    "score = model.evaluate(test_data, test_labels_category, verbose=1)\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a2414c",
   "metadata": {},
   "source": [
    "# Second CNN Model - Using bucket paw scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "9bdb44a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "247/247 [==============================] - 8s 28ms/step - loss: 0.5331 - rmse: 0.5793 - mae: 0.4355 - mape: 343367328.0000 - val_loss: 0.2822 - val_rmse: 0.3019 - val_mae: 0.1528 - val_mape: 59382148.00005248\n",
      "Epoch 2/3\n",
      "247/247 [==============================] - 6s 26ms/step - loss: 0.3140 - rmse: 0.3545 - mae: 0.2403 - mape: 149989936.0000 - val_loss: 0.2759 - val_rmse: 0.3007 - val_mae: 0.1633 - val_mape: 71237664.0000\n",
      "Epoch 3/3\n",
      "247/247 [==============================] - 6s 25ms/step - loss: 0.2804 - rmse: 0.3134 - mae: 0.1897 - mape: 99568704.0000 - val_loss: 0.2687 - val_rmse: 0.3001 - val_mae: 0.1744 - val_mape: 83679408.0000\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.2687 - rmse: 0.3001 - mae: 0.1744 - mape: 83679408.0000\n",
      "Test accuracy: 0.3000822365283966\n"
     ]
    }
   ],
   "source": [
    "# model from kaggle page \n",
    "# https://www.kaggle.com/alexteboul/tutorial-part-3-cnn-image-modeling-1\n",
    "\n",
    "# this accuracy seems ot be higher we could mess around with different filters/padding etc\n",
    "shape = (train_data.shape[1],train_data.shape[2], train_data.shape[3])\n",
    "inputs = tf.keras.Input(shape=shape)\n",
    "\n",
    "#start off with x just being those inputs\n",
    "x = inputs\n",
    "\n",
    "x = tf.keras.layers.Conv2D(filters = 16, kernel_size = (7,7), strides = (2,2), padding='valid', kernel_initializer='he_normal', kernel_regularizer=l2(0.0005), activation = 'relu')(x)\n",
    "x = tf.keras.layers.Conv2D(filters = 32, kernel_size = (3,3), padding='same', activation = 'relu')(x)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "x = tf.keras.layers.Conv2D(filters = 32, kernel_size = (3,3), strides = (2,2), padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(0.0005), activation = 'relu')(x)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "x = tf.keras.layers.Dropout(0.25)(x)\n",
    "\n",
    "\n",
    "x = tf.keras.layers.Conv2D(filters = 64, kernel_size = (3,3), padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(0.0002), activation = 'relu')(x)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "x = tf.keras.layers.Conv2D(filters = 64, kernel_size = (3,3), strides = (2,2), padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(0.0005), activation = 'relu')(x)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "x = tf.keras.layers.Dropout(0.25)(x)\n",
    "\n",
    "x = tf.keras.layers.Conv2D(filters = 128, kernel_size = (3,3), padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(0.0002), activation = 'relu')(x)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = tf.keras.layers.Conv2D(filters = 128, kernel_size = (3,3),padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(0.0002), activation = 'relu')(x)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "x = tf.keras.layers.Dropout(0.25)(x)\n",
    "\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "x = tf.keras.layers.Dense(512, activation = \"relu\")(x)\n",
    "x = tf.keras.layers.Dropout(0.5)(x)\n",
    "\n",
    "output = tf.keras.layers.Dense(1)(x)\n",
    "\n",
    "model = tf.keras.Model(inputs = inputs, outputs = output)\n",
    "\n",
    "model.compile(\n",
    "    loss = 'mse', \n",
    "    optimizer = 'Adam', \n",
    "    metrics = [tf.keras.metrics.RootMeanSquaredError(name=\"rmse\"), \"mae\", \"mape\"])\n",
    "\n",
    "data_augmentation = ImageDataGenerator(\n",
    "    rotation_range = 15, \n",
    "    zoom_range = 0.15,\n",
    "    width_shift_range = 0.2, \n",
    "    height_shift_range = 0.2, \n",
    "    shear_range = 0.1,\n",
    "    horizontal_flip = True, \n",
    "    fill_mode = \"nearest\")\n",
    "history = model.fit(\n",
    "    data_augmentation.flow(train_data,train_labels_bins_category,batch_size=32),\n",
    "    validation_data = (test_data, test_labels_bins_category),\n",
    "    steps_per_epoch = len(blurred) // 32,\n",
    "    epochs = 3\n",
    ")\n",
    "score = model.evaluate(test_data, test_labels_bins_category, verbose=1)\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9f8c46",
   "metadata": {},
   "source": [
    "# Trying other models (Naive Bayes & KNN) - raw paw scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "fb97e7b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------  K Nearest Neighbor  ----------\n",
      "accuracy with k= 1  :  0.0050200803212851405\n",
      "accuracy with k= 2  :  0.015060240963855422\n",
      "accuracy with k= 3  :  0.0321285140562249\n",
      "accuracy with k= 5  :  0.01606425702811245\n",
      "accuracy with k= 7  :  0.01606425702811245\n",
      "---------- Naive Bayes  ----------\n",
      "Bernoulli Model accuracy: 0.0231\n"
     ]
    }
   ],
   "source": [
    "# Other model attempts:\n",
    "# have to reshape the data to input it into a different model\n",
    "train_size = train_data.shape[0]\n",
    "train_shape = train_data.shape[1]*train_data.shape[2]*train_data.shape[3]\n",
    "\n",
    "test_size = test_data.shape[0]\n",
    "test_shape = test_data.shape[1]*test_data.shape[2]*test_data.shape[3]\n",
    "\n",
    "train_data_temp = train_data.reshape(train_size,train_shape)\n",
    "test_data_temp = test_data.reshape(test_size,test_shape)\n",
    "print('-'*10, ' K Nearest Neighbor ', '-'*10)\n",
    "for k in [1,2,3,5,7]:\n",
    "    knnmodel = KNeighborsClassifier(n_neighbors=k)\n",
    "    knnmodel.fit(train_data_temp, train_labels_raw)\n",
    "    test_predicted_labels = knnmodel.predict(test_data_temp)\n",
    "    print(\"accuracy with k=\", k, \" : \", metrics.accuracy_score(test_labels_raw, test_predicted_labels))\n",
    "\n",
    "\n",
    "print('-'*10, 'Naive Bayes ', '-'*10)\n",
    "naive_bayes_model1 = BernoulliNB()\n",
    "naive_bayes_model1.fit(train_data_temp, train_labels_raw)\n",
    "print ('Bernoulli Model accuracy: %3.4f' %naive_bayes_model1.score(test_data_temp, test_labels_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "0226c375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------  K Nearest Neighbor  ----------\n",
      "accuracy with k= 1  :  0.2740963855421687\n",
      "accuracy with k= 2  :  0.2740963855421687\n",
      "accuracy with k= 3  :  0.2520080321285141\n",
      "accuracy with k= 5  :  0.2740963855421687\n",
      "accuracy with k= 7  :  0.2740963855421687\n",
      "---------- Naive Bayes  ----------\n",
      "Bernoulli Model accuracy: 0.2741\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('-'*10, ' K Nearest Neighbor ', '-'*10)\n",
    "for k in [1,2,3,5,7]:\n",
    "    knnmodel = KNeighborsClassifier(n_neighbors=k)\n",
    "    knnmodel.fit(train_data_temp, train_labels_bin)\n",
    "    test_predicted_labels = knnmodel.predict(test_data_temp)\n",
    "    print(\"accuracy with k=\", k, \" : \", metrics.accuracy_score(test_labels_bin, test_predicted_labels))\n",
    "\n",
    "\n",
    "print('-'*10, 'Naive Bayes ', '-'*10)\n",
    "naive_bayes_model1 = BernoulliNB()\n",
    "naive_bayes_model1.fit(train_data_temp, train_labels_bin)\n",
    "print ('Bernoulli Model accuracy: %3.4f' %naive_bayes_model1.score(test_data_temp, test_labels_bin))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
